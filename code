import requests as R 
from bs4 import BeautifulSoup as BS 
from time import sleep 
import pandas as pd 


data = []
# создаем пустой список, куда будем добавлять данные

for p in range (1,3): #тут 2 страницы
   print(p) #выводит номер страницы, которую парсит
   url = f'https://stopgame.ru/topgames?rating=izumitelno&p={p}' 
   #url = f'https://stopgame.ru/games?p={p}'
   # переписываем url - вместо номера стр. счетчик цикла в фигурных скобках {p}, чтобы собирать со всех страниц сайта, вывод в строчку (f'')
   r = R.get(url)
   sleep(3) # после первого запроса код ждет 3с.
   soup = BS(r.content, "html.parser")#передаем библиотеке BeautifulSoup извлеченный код страницы по указанному ранее URL, можно использовать как метод text, так и content

   games = soup.findAll('div', class_ = 'item game-summary game-summary-horiz')
   #находим все игры на одной странице с 1-м в иерархии классом
   #print(len(games))#30 игр на одной странице

   for game in games:
           name = game.select('.item .caption > a')[0].text
# извлекает НАЗВАНИЕ ссылки ПОД тегом "а". Важны пробелы. Сначала извлекает список, к которому невозможно применить ф-ю text, поэтому важно вытянуть содержание списка в текстом формате, а затем применять ф-ю
           link = 'https://stopgame.ru' + game.find('a', class_='link').get('href')
           platforms = game.find('div', class_='game-spec').find('span', class_='value').text
           jenre = game.findAll('div', class_='game-spec')[1].find('span', class_='value').text
           rate = game.find('div', class_='score').text.strip()
           date = game.findAll('div', class_='game-spec')[2].find('span', class_='value').text
   # цикл для извлечения этих 30 значений,
   # ('div', class_ = 'item game-summary game-summary-horiz') заменяем на game (итератор цикла), т.к. мы присвоили этот путь переменной games
           data.append([name,link,platforms,jenre,rate,date])
   #заполняем список
print(data)

header = ['name','link','platforms','jenre','rate','date']
#создаем переменную с колонками будущей таблицы
df = pd.DataFrame(data, columns=header) #pandas
#создаем таблицу
df.to_csv('C:/Users/Ioan/Desktop/Parsing_games.csv', sep = ';', encoding = 'utf8') #выгружаем в csv
